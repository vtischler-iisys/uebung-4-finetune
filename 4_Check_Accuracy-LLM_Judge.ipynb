{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9448617-4f6b-438b-8b92-1163afc6fd65",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://nexus.iisys.de/repository/ki-awz-pypi-group/simple, https://pypi.org/simple\n",
      "Collecting datasets\n",
      "  Using cached datasets-4.1.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from datasets) (2.2.6)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /opt/conda/lib/python3.12/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /opt/conda/lib/python3.12/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (from datasets) (2.3.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.12/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /opt/conda/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2025.9.0)\n",
      "Collecting huggingface-hub>=0.24.0 (from datasets)\n",
      "  Using cached huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /opt/conda/lib/python3.12/site-packages (from aiosignal>=1.4.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (4.15.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub>=0.24.0->datasets)\n",
      "  Using cached hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Using cached datasets-4.1.1-py3-none-any.whl (503 kB)\n",
      "Using cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Using cached huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n",
      "Using cached hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "Using cached xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
      "Installing collected packages: xxhash, multiprocess, hf-xet, huggingface-hub, datasets\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [datasets]4/5\u001b[0m [datasets]ce-hub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed datasets-4.1.1 hf-xet-1.1.10 huggingface-hub-0.35.3 multiprocess-0.70.16 xxhash-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88697ac4-04f8-4fed-876a-d447337c0dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://nexus.iisys.de/repository/ki-awz-pypi-group/simple, https://pypi.org/simple\n",
      "Collecting openai\n",
      "  Downloading openai-2.6.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.12/site-packages (from openai) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from openai) (0.28.1)\n",
      "Collecting jiter<1,>=0.10.0 (from openai)\n",
      "  Downloading jiter-0.11.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.12/site-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/conda/lib/python3.12/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/conda/lib/python3.12/site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/conda/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Downloading openai-2.6.0-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jiter-0.11.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (358 kB)\n",
      "Installing collected packages: jiter, openai\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [openai]2m1/2\u001b[0m [openai]\n",
      "\u001b[1A\u001b[2KSuccessfully installed jiter-0.11.1 openai-2.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e937669d-763d-442b-a8ed-aa4c66d2d099",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Schnowflake/playing_cards_excercise\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72ce951d-ff33-4952-bf47-922eb6e5b753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=720x720 at 0x7F303CAF8EF0>, 'qa': [{'question': 'what cards are in the image?', 'answer': '4 of club, 5 of spade, 8 of spade'}]}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57be76c3-5780-4b39-9707-333f4bdbbf0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e7e59b-95bf-4955-9a07-f8f3e07d9c9e",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39905c78-ddc1-4b96-837c-22dd0b4e1c87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.10.9: Fast Gemma3 patching. Transformers: 4.55.4.\n",
      "   \\\\   /|    NVIDIA H100 PCIe. Num GPUs = 1. Max memory: 79.189 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 9.0. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Gemma3 does not support SDPA - switching to fast eager.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Gemma3ForConditionalGeneration(\n",
       "  (model): Gemma3Model(\n",
       "    (vision_tower): SiglipVisionModel(\n",
       "      (vision_model): SiglipVisionTransformer(\n",
       "        (embeddings): SiglipVisionEmbeddings(\n",
       "          (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
       "          (position_embedding): Embedding(4096, 1152)\n",
       "        )\n",
       "        (encoder): SiglipEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0-26): 27 x SiglipEncoderLayer(\n",
       "              (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "              (self_attn): SiglipAttention(\n",
       "                (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): SiglipMLP(\n",
       "                (activation_fn): PytorchGELUTanh()\n",
       "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (multi_modal_projector): Gemma3MultiModalProjector(\n",
       "      (mm_soft_emb_norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "      (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
       "    )\n",
       "    (language_model): Gemma3TextModel(\n",
       "      (embed_tokens): Gemma3TextScaledWordEmbedding(262208, 2560, padding_idx=0)\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x Gemma3DecoderLayer(\n",
       "          (self_attn): Gemma3Attention(\n",
       "            (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
       "            (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "            (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "            (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
       "            (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "            (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "          )\n",
       "          (mlp): Gemma3MLP(\n",
       "            (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "            (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "            (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
       "            (act_fn): PytorchGELUTanh()\n",
       "          )\n",
       "          (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "        )\n",
       "        (2): Gemma3DecoderLayer(\n",
       "          (self_attn): Gemma3Attention(\n",
       "            (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
       "            (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
       "            (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "            (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "          )\n",
       "          (mlp): Gemma3MLP(\n",
       "            (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "            (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "            (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
       "            (act_fn): PytorchGELUTanh()\n",
       "          )\n",
       "          (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "        )\n",
       "        (3-5): 3 x Gemma3DecoderLayer(\n",
       "          (self_attn): Gemma3Attention(\n",
       "            (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
       "            (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "            (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "            (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
       "            (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "            (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "          )\n",
       "          (mlp): Gemma3MLP(\n",
       "            (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "            (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "            (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
       "            (act_fn): PytorchGELUTanh()\n",
       "          )\n",
       "          (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "        )\n",
       "        (6-33): 28 x Gemma3DecoderLayer(\n",
       "          (self_attn): Gemma3Attention(\n",
       "            (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
       "            (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "            (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "            (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
       "            (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "            (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "          )\n",
       "          (mlp): Gemma3MLP(\n",
       "            (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
       "            (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
       "            (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
       "            (act_fn): PytorchGELUTanh()\n",
       "          )\n",
       "          (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "        )\n",
       "      )\n",
       "      (norm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "      (rotary_emb): Gemma3RotaryEmbedding()\n",
       "      (rotary_emb_local): Gemma3RotaryEmbedding()\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=262208, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth import get_chat_template\n",
    "from unsloth import FastVisionModel\n",
    "\n",
    "\"\"\"\n",
    "model, processor = FastVisionModel.from_pretrained(\n",
    "    model_name=\"lora_model_cards\",  # YOUR MODEL YOU USED FOR TRAINING\n",
    "    load_in_4bit=True,  # Set to False for 16bit LoRA\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "model, processor = FastVisionModel.from_pretrained(\n",
    "    \"unsloth/gemma-3-4b-it\",\n",
    "    load_in_4bit = True, # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",
    ")\n",
    "\n",
    "processor = get_chat_template(\n",
    "    processor,\n",
    "    \"gemma-3\"\n",
    ")\n",
    "\n",
    "FastVisionModel.for_inference(model)  # Enable for inference!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aba64ad0-b415-4653-ad12-d0910d75a1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from openai import OpenAI\n",
    "\n",
    "def VLM_Call(image):\n",
    "\n",
    "    instruction = \"What cards are in the image?\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": instruction}],\n",
    "        }\n",
    "    ]\n",
    "    input_text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "\n",
    "\n",
    "    inputs = processor(\n",
    "        image,\n",
    "        str(input_text),  \n",
    "        add_special_tokens=False,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    result = model.generate(**inputs, max_new_tokens = 128,\n",
    "                            use_cache=True, temperature = 1.0, top_p = 0.95, top_k = 64)\n",
    "    \n",
    "    text_back = processor.decode(result[0], skip_special_tokens=True)\n",
    "    after_model = re.split(r'(?i)model', text_back, maxsplit=1)[-1].strip()\n",
    "    \n",
    "    return  after_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b50e6fa0-ce70-4f42-935e-5ad05e38b76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_Judge(groundtruth, vlm_answer):\n",
    "    client = OpenAI(\n",
    "        base_url='https://chat-1.ki-awz.iisys.de/api/',\n",
    "        api_key='sk-ae96b9eb838f49858d7f977a10294374',  # \n",
    "    )\n",
    "\n",
    "    system_prompt = f\"\"\"You are a data-scientist and need to compare the ground-truth to the answer given by a model.\n",
    "    You are specificially task to see if the visual model has guessed the names of playing cards correctly\n",
    "\n",
    "    Write \"correct\" if the model is correct. The order of them is not important.\n",
    "    If any of the cards are wrong, write \"wrong!\" instead.\n",
    "    Write nothing else.\n",
    "    \n",
    "    The correct answer (groundtruth) is: {groundtruth}\n",
    "    \n",
    "    Here´s the model´s answer:\"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"lisa-v40-rc2-gpt-oss120b\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": vlm_answer}\n",
    "        ],\n",
    "        temperature=0.1\n",
    "    )\n",
    "\n",
    "    antwort = response.choices[0].message.content\n",
    "\n",
    "    return antwort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f16f6c59-51a1-4ad9-93b0-c9e2e678900d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "correct!\n",
      "correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "correct!\n",
      "Not correct!\n",
      "correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "correct!\n",
      "Not correct!\n",
      "correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "correct!\n",
      "Not correct!\n",
      "correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "correct!\n",
      "Not correct!\n",
      "correct!\n",
      "correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "correct!\n",
      "correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "correct!\n",
      "correct!\n",
      "Not correct!\n",
      "correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "correct!\n",
      "correct!\n",
      "Not correct!\n",
      "correct!\n",
      "correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "correct!\n",
      "Not correct!\n",
      "correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "correct!\n",
      "correct!\n",
      "Not correct!\n",
      "correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "correct!\n",
      "Not correct!\n",
      "correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "correct!\n",
      "Not correct!\n",
      "correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "correct!\n",
      "correct!\n",
      "correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "correct!\n",
      "correct!\n",
      "Not correct!\n",
      "Not correct!\n",
      "52\n"
     ]
    }
   ],
   "source": [
    "total_correct = 0\n",
    "\n",
    "data_to_safe = []\n",
    "\n",
    "for row in dataset:\n",
    "    vlm_answer = VLM_Call(row[\"image\"])\n",
    "    judge_answer = call_Judge(vlm_answer, row[\"qa\"][0][\"answer\"])\n",
    "    if \"correct\" in judge_answer:\n",
    "        total_correct += 1\n",
    "        print(\"correct!\")\n",
    "    else:\n",
    "        #print(row[\"qa\"][0][\"answer\"])\n",
    "        #print(vlm_answer)\n",
    "        print(\"Not correct!\")\n",
    "\n",
    "    data = {}\n",
    "    data[\"Groundtruth\"] = row[\"qa\"][0][\"answer\"]\n",
    "    data[\"VLM\"] = vlm_answer\n",
    "    data[\"Judge\"] = judge_answer\n",
    "\n",
    "    data_to_safe.append(data)\n",
    "\n",
    "print(total_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bae3eab-5146-4f73-93eb-aa60d087b49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "171/200 korrekt - Finetuned\n",
    "6/200 korrekt - nicht finetuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "215b50b7-1f5b-4322-a71b-98c701c3f78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame.from_dict(data_to_safe)\n",
    "df.to_csv(\"Results_non_tuned_easyALG.csv\", sep=',', encoding='utf-8', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ea3c68-452a-4e7f-89ba-3f832a921e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
